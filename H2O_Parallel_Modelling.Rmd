
---
title: "H2O_Parallel_Modelling"
output:
  html_document:
    toc: true
---


```{r}
%md
# Modelling Script
This script performs the below functionalities  : 

1.  Set Up H2O Context & MLFlow in DataBricks
2.  Runs various algorithms in H2O GRID parallely for each Product on Train Data
3.  Selects the best model based on model selection criteria  
4.  Predict the test data using selected model 
5.  Save H2O Models in Databricks File System using MOJO
6.  Obtain Model Coeffecients 
7.  Push H2OFrame to SQL  Using Scala   
```


```{r}
# load Libraries
library(SparkR)
library(data.table)
library(mlflow)
install_mlflow()
library(stringr)
library(tidyverse)
library(parallel)
```


```{r}
install.packages("SparkR", dependencies = TRUE)
```


```{r}
# Set Up H2O Context
install.packages("h2o", type = "source", repos = "https://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/3/R") 
install.packages("rsparkling", type = "source", repos = "http://h2o-release.s3.amazonaws.com/sparkling-water/spark-2.4/3.30.0.3-1-2.4/R") 
install.packages("sparklyr")

library("sparklyr")
library("rsparkling")
library("h2o")

sc <- spark_connect(method = "databricks")
hc <- H2OContext.getOrCreate() 
```


```{r}
# Set Up ML Flow 
experiment_location  =  "/Archive/TEST_MLFLOW"
### Start ML Flow & Track
RunTimeStamp <- Sys.time()
mlflow_set_experiment(experiment_location)
run <- mlflow_start_run()

RUNID  =  run$run_id
print(experiment_location)
print(RunTimeStamp)
print(RUNID)
```


```{r}
# Import Input data 
mrd <-  as.h2o(read.csv("/dbfs/FileStore/tables/Sampledata.csv",header=TRUE))
beta <-  as.h2o(read.csv("/dbfs/FileStore/tables/Samplebeta.csv",header=TRUE))
head(mrd)
```


```{r}
# Data Preparation for Modelling
# Make global variable for name of Target Variable in Data 
assign("targetVar","Target")

#Grab list of unique products
modIDS <- h2o.unique(as.factor(mrd$Product))
modIDS <-  paste0(as.data.table(modIDS)$C1)

print(head(modIDS))
print(length(modIDS))
```


```{r}
#Split data by Product and assign to h2o backend    

for (i in 1:length(modIDS)){
  #split Entire dataset for prediction 
  h2o.assign(mrd[mrd$Product==modIDS[i],], key=paste0("allSplitDat",RUNID,i))  
  # Split Training data for Model Training 
  h2o.assign(mrd[mrd$Product==modIDS[i] & mrd$TRAIN_IND==1,], key=paste0("trainDat",RUNID,i))
  # Split beta  for parallel modelling 
  h2o.assign(beta, key=paste0("betaSplit",RUNID,i))
}
```


```{r}
# Model Function
h2o.runGLM <- function (workerCores, driverCores, dataDict) 
{
    modelGrids = mclapply(1:length(modIDS), function(i) {
        h2o.grid(algorithm        = "glm", 
                 grid             = paste0(dataDict$gridName, i), 
                 x                = c(dataDict$xVars), 
                 y                = get("targetVar"), 
                 training_frame   = h2o.getFrame(paste0(dataDict$trainDat, i)), 
                 family           = "gaussian", 
                 beta_constraints = h2o.getFrame(paste0(dataDict$betas, i)), 
                 parallelism      = driverCores,
                 hyper_params     = dataDict$inHyperParams)
    },mc.cores = workerCores)
  return(modelGrids)
}
```


```{r}
# Model Training
#Run Models 
driverCores = 20
workerCores = 64
xVars<-  names(mrd)[grepl(x=names(mrd),pattern="X",ignore.case=TRUE)]
xVars

glmDataDict <- list(xVars        = xVars,
                    gridName     = paste0("glmGrid", RUNID),
                    trainDat     = paste0("allSplitDat",RUNID),
                    betas     = paste0("betaSplit", RUNID),
                    inHyperParams = list(alpha = c(0,0.5,1))
                   )

modelGrids <- h2o.runGLM(workerCores, driverCores, glmDataDict)
head(modelGrids)
```


```{r}
%md

##### Select best model #####
```


```{r}
# H2O Function to select best model based on  Evaluation Metric  
h2o.selectBestModel <- function(inGrid, selectionCriterion){
  #Select best model
  if (selectCriterion %in% c("MAE","RMSE")){
  gridPerf <- h2o.getGrid(grid_id = inGrid,
                             sort_by = selectCriterion,
                             decreasing = FALSE)
  
  bestModel <- gridPerf@model_ids[[1]]
  }else if (selectCriterion %in% c("auc")){
    gridPerf <- h2o.getGrid(grid_id = inGrid,
                               sort_by = selectCriterion,
                               decreasing = TRUE)

    bestModel <- gridPerf@model_ids[[1]]
  }
  return(bestModel)
}
```


```{r}
#Select best model based on train accuracy
gridIDS <- paste0("glmGrid",RUNID,1:length(modIDS))
selectCriterion = "RMSE"
selectedModels <- mclapply(gridIDS,h2o.selectBestModel)
selectedModels <- unlist(selectedModels)
head(selectedModels)
```


```{r}
#Subset model objects for final models
modelObjects <- mclapply(selectedModels, function(id) { h2o.getModel(id)}, mc.cores=20)
names(modelObjects) <- modIDS
print(length(modelObjects))
head(modelObjects)
```


```{r}
%md

##### Score test #####
```


```{r}
#H2O Function to predict on test data 
h2o.score <- function (i, data, inObjects) 
{
    modID <- modIDS[[i]]
    oneObject <- inObjects[[modID]]
    predict <- h2o.predict(oneObject, h2o.getFrame(paste0(data,i)))
    predict <- h2o.cbind(h2o.getFrame(paste0(data, i)),predict)
    return(predict)
}
```


```{r}
#Score predictions
listPreds <- mclapply(1:length(modIDS),h2o.score,
                      paste0("allSplitDat",RUNID), modelObjects,
                      mc.cores=20) 
```


```{r}
#Tidy predictions
bindPreds <- do.call(h2o.rbind,listPreds)
head(bindPreds)
```


```{r}
# H2O function to obtain elasticities
h2o.getCoefsummary <- function (inModelObject) 
{
    coefs <- as.data.table(inModelObject@model$coefficients_table)
    coefs <- coefs[!str_detect(coefs$names, "MODEL_ID", negate = FALSE), ]
    coefs <- coefs[!(names %in% c("Intercept")), ]
    coefs$MODEL_ID <- names(inModelObject)
   coefs$Model_Type = inModelObject@model$model_summary$regularization
    return(coefs)
}
```


```{r}
#Generate elasticities
coefs = lapply(modelObjects,h2o.getCoefsummary)
allCoefs <- bind_rows(coefs, .id = "meta_information")
setnames(allCoefs,"meta_information","MODEL_ID")
display(allCoefs)
```


```{r}
%md

#####Save Model Objects #####
```


```{r}
# Save H2O Models as MOJO
saveh2oModels <- function(allmodelObjects,outPath){
  for (i in 1:length(allmodelObjects)){
    thisModel <- h2o.getModel(allmodelObjects[[i]]@model_id)
    thisOutPath <- paste0(outPath,names(modelObjects[i]))
    mojo_destination <- h2o.save_mojo(thisModel, path = thisOutPath, force=TRUE)
    print(mojo_destination)
  } 
}
```


```{r}
# Databricks File System Path to Save 
dbfsPath  = "dbfs:/mnt/adls/Test/Databricks/TestModels"
```


```{r}
saveh2oModels(modelObjects,dbfsPath)
```


```{r}
# Export H2O frame to DBFS Path 
dataPath =  paste0(dbfsPath,"/DATA_OUTPUT.csv")
h2o.exportFile(bindPreds, path = dataPath, force=TRUE)
```


```{r}
# Create a temporary view in Spark
SparkR::createOrReplaceTempView(x=createDataFrame(as.data.table(dataPath)),"dataPath")
```


```{r}
%scala
 // Define SQL JDBC connection parameters  
val sqlEnvironment = "XYZ";
val  user = "XYZ";
val password = "XYZ";
val database = "XYZ";
val server = "abc.net.xyz.123";
val jdbcUrlScala = "jdbc:sqlserver://"+server+";database="+database+";user="+user+";password="+password
```


```{r}
%scala
import org.apache.spark.sql.Row


def grabSparkRParameters(inParam:String) : String = {   
val dftmp = spark.sql(s"SELECT " + inParam + " FROM " + inParam)
val outParam1 = dftmp.select(inParam)
                .collectAsList()
val outParam2 = outParam1.get(0).toString()
val outParam3 = outParam2.filter(!"[".contains(_))
val outParam = outParam3.filter(!"]".contains(_))
  
  return outParam
}
```


```{r}
%scala
// Push data to SQL Table  "SAMPLE_DATA" 
import com.microsoft.azure.sqldb.spark.bulkcopy.BulkCopyMetadata
import com.microsoft.azure.sqldb.spark.config.Config
import com.microsoft.azure.sqldb.spark.connect._
import scala.collection.mutable.ListBuffer
import scala.collection.JavaConversions._
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql._
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._
import java.util.Properties
import java.sql.DriverManager


def appendDataToSqlTable(df:DataFrame, tableName:String){ 
  
  val jdbcUsername = user
  val jdbcPassword = password
  val driverClass = "com.microsoft.sqlserver.jdbc.SQLServerDriver"
  
  val connectionProperties = new Properties()

  connectionProperties.put("user", s"${jdbcUsername}")
  connectionProperties.put("password", s"${jdbcPassword}")
  connectionProperties.setProperty("Driver", driverClass)
  
  df.write.mode(SaveMode.Append).jdbc(jdbcUrlScala, tableName, connectionProperties)
}

//Obtain outpath
val outPath = grabSparkRParameters("dataPath")

//Read in data
val df = spark.read.format("csv")
 .option("header", "true").option("inferSchema", "true")
 .load(outPath)

// Append Data
appendDataToSqlTable(df,"SAMPLE_DATA")
```


```{r}
%md

##### MLFLOW_TRACKER #####
```


```{r}
# Create  ML FLOW Tracker Table
print(Sys.time()-RunTimeStamp)
mlflow_log_param("Time", Sys.time()-RunTimeStamp)
runTable <- data.table(timeStamp = RunTimeStamp, runID = run$run_id, uri=mlflow_get_tracking_uri(), experiment_location=experiment_location, experiment_id=run$experiment_id,stage= "Test Modelling Script")
display(runTable)
mlflow_end_run()
```

